---
layout: post
title: DevFest π 2024 후기 / Responsible AI in Action
author-id: antraxmin
feature-img: "https://velog.velcdn.com/images/antraxmin/post/ed6d56e9-ce90-4805-8d1a-4d817a578903/image.png"
tags: [컨퍼런스/헹사]
date: 2024-12-28 00:00:00
---

> 요즘 AI가 대세라지만, 과연 우리는 AI를 제대로 이해하고 활용하고 있을까? 

이런 고민을 안고 이번 2024 GDG DevFest π 오프라인 행사에 참석했다. 올해 마지막 DevFest는 GDG Pangyo와 연세대, 이화여대 GDGoC 커뮤니티와 함께 'Responsible AI in Action'이라는 주제로 진행되었다. 이날 행사는 오후 1시부터 4시까지 진행되었는데, GDG 오거나이저의 오프닝을 시작으로 다채로운 세션들이 이어졌다. 

![](https://velog.velcdn.com/images/antraxmin/post/d9a496eb-35e8-4a78-be53-85ff88e9f8d5/image.png)

<br />

## GDG x GDGoC 데모부스 
행사장에 들어가기 전, 로비에 전시된 데모부스를 통해 다양한 서비스들을 직접 체험해볼 수 있었다. Sionic AI 같은 떠오르는 스타트업부터 연세대와 이화여대 GDGoC의 프로젝트들까지 다양한 서비스들이 눈길을 끌었다. 특히 연세대 팀의 캠퍼스 길찾기 서비스가 인상적이었는데, 단순히 상용 지도 서비스를 활용한 게 아니라 **OSM(OpenStreetMap)을 기반으로 자체 개발한 솔루션**이었다. 지도에 나와있지 않은 캠퍼스 내 지름길이나 샛길까지 꼼꼼하게 매핑해 실제 학교를 다니는 학생들에게 유용한 서비스를 만들었다는 점이 돋보였다.

![](https://velog.velcdn.com/images/antraxmin/post/b5e9c0fd-2895-4c38-a28c-4b3c4dfb237f/image.png)

AI 챗봇과의 연동 기능도 신기했다. "도서관에서 공대 건물까지 비 안 맞고 가는 길 알려줘", "수업 들으러 가는데 카페에 들렀다 가고 싶어" 같은 자연스러운 대화형 명령으로 경로를 찾을 수 있게 했다. 실제로 이런 기능이 있으면 좋겠다고 생각만 했었는데 직접 구현해낸 게 인상적이었다.

실제로 나 역시 우리 대학교 학우들을 위해 캠퍼스맵 서비스를 개발했었는데, 여러 면에서 실력 차이를 느꼈다. 특히 OSM을 활용해 자체 지도 데이터를 구축하고 AI와 연동해 자연어 처리까지 가능하게 한 기술력이 놀라웠다. 내가 만든 서비스는 단순히 강의실 번호를 통해 해당 위치를 검색할 수 있는 기능에 그쳤지만, 이분들은 좀더 복잡한 사용자 경험을 깊이 고민하고 구현해낸 흔적이 보였다. "카페 들렀다 가기", "도서관에서 가장 가까운 프린터" 같은 실용적인 기능들은 내가 미처 생각하지 못한 부분이었다. 이런 부분들을 참고해서 기존 서비스 기능 개선을 제대로 해보고 싶다는 생각이 들었다. 

<br />

## Keynote
키노트에서 오순영 공동의장님은 최근 AI 안전성에 대한 글로벌 동향부터 시작해 깊이 있는 통찰을 공유해주셨다. 특히 2023년 11월 영국에서 있었던 AI Safety Summit에서 논의된 **Frontier AI의 정의와 안전성 관리 체계**에 대해 상세히 설명해주셨는데, 각 국가별 접근 방식의 차이가 흥미로웠다. 미국의 자율규제 원칙부터 EU의 강력한 법안과 벌금 체계, 중국의 정부 통제, 그리고 우리나라의 기본법 추진까지 각국의 특성이 잘 반영된 정책들을 비교해볼 수 있었다. 특히 인상 깊었던 것은 **Frontier of Advanced AI의 안전성 강화를 위한 기업들의 의무사항**이었다. 강력한 AI 시스템의 능력과 영향력 평가 및 공개, 제3자에 의한 안전성 평가 체계 구축 등 구체적인 방안들이 제시되었는데, 국내에서는 네이버가 이런 노력의 선두에 있다고 한다. 워터마킹과 같은 기술적 솔루션부터 정책적 대응까지 AI 안전성을 위한 다각도의 접근이 필요하다는 점이 인상적이었다.

![](https://velog.velcdn.com/images/antraxmin/post/ba64865b-b6f7-4184-a5f8-175986059a19/image.png)


또한 AI Alignment에 대한 논의도 깊이 있게 다뤄졌다. AI 시스템이 어떤 가치에 정렬(Align)되어야 하는가라는 근본적인 질문부터 시작해서 단순히 기술적인 차원의 RLHF를 넘어서는 고민이 필요하다는 점이 강조되었다. AI 시스템이 만들어내는 문제 해결 방식이 인간의 가치와 목표에 얼마나 잘 부합하는지, 그리고 이를 위한 윤리적, 정책적, 법적 고려사항들이 무엇인지 진지하게 생각해볼 수 있는 시간이었다.

특히 눈여겨볼 만했던 것은 **인간과 AI의 협업 효과에 대한 실증적인 연구 결과**였다. 106가지 실험을 통한 370개의 메타 분석 결과는 우리의 일반적인 가정을 뒤집는 흥미로운 내용을 담고 있었다. AI를 단순히 프로세스에 통합하는 것이 항상 좋은 성과로 이어지지는 않으며, **오히려 평균적인 인간과 AI 시스템의 조합이 각각 독립적으로 일할 때보다 성과가 떨어질 수 있다는 점**은 많은 생각거리를 던져주었다. 이는 AI 도입에 있어 보다 신중하고 전략적인 접근이 필요하다는 것을 시사한다.

결론적으로 AI 시대에 필요한 인간 전문가는 **특정 문제를 정확히 타겟팅할 수 있는 능력**을 갖춰야 한다는 점이 강조되었다. 단순한 지식의 축적을 넘어서는 지혜의 필요성, 그리고 소프트스킬의 중요성이 더욱 부각되었다. 이는 앞으로 AI와 함께할 미래에서 우리가 어떤 역량을 갖추고 어떤 방향으로 발전해야 할지에 대한 중요한 시사점을 제공해주었다. AI가 발전할수록 역설적으로 더욱 중요해지는 것이 바로 인간다움, 즉 문제를 바라보는 통찰력과 지혜, 그리고 이를 바탕으로 한 판단력이라는 점을 이번 키노트를 통해 다시 한 번 확인할 수 있었다.

<br />

## Responsible AI와 국내 산업 경쟁력 강화를 위한 규제 전망
'Responsible AI와 국내 산업 경쟁력 강화를 위한 규제 전망' 세션에서는 **AI 규제를 바라보는 새로운 시각**을 제시해주셨다. 최정윤 연구원님은 정부의 입법을 지원하는 거의 유일한 기관인 **한국법제연구원**의 관점에서 AI 규제가 직면한 현실적인 도전 과제들을 짚어주셨다. 특히 법을 만드는 사람들과 AI 기술을 개발하는 사람들 사이의 간극이 크다는 점을 강조하셨는데, 실제로 법을 만드는 분들은 AI 기술을 깊이 이해하기 어렵고, 기술을 개발하는 분들은 규제를 잘 모르는 상황이라고 한다. 이런 배경에서 한국법제연구원은 KBI, STEPI와 함께 'AI 시대의 경쟁력 강화를 위한 규제 연구'를 진행했다고 한다. 사회 현상을 지켜보고 법을 만들어야 하는 법학의 특성상 AI처럼 빠르게 발전하는 기술 분야를 규제하는 것은 쉽지 않은 도전이라고.

![](https://velog.velcdn.com/images/antraxmin/post/5c75dd18-6b3a-4f40-bd70-d23af3922f88/image.png)


글로벌 동향을 보면 각 국가별로 접근 방식이 상당히 다른데, 미국은 시장 자율규제 원칙을, EU는 강력한 법안과 벌금 체계를, 중국은 정부 통제를 중심으로 하고 있다고 한다. 바이든 정부의 AI 행정명령도 흥미로운 사례였는데, **입법부를 거치지 않고도 효력을 발생시킬 수 있는 독특한 형태의 규제**라고 설명해주셨다. 우리나라의 경우 최근 통과된 AI 기본법이 과기부 중심으로 만들어져서 금융이나 의료, 제조업 등 다른 부처 소관의 AI 발전 영역을 어떻게 포섭할 것인지가 과제로 남아있다고 하셨다. 특히 기본법이라는 형태를 택한 것도 의미가 있는데, 정부가 바뀌더라도 AI 관련 정책의 프레임을 유지하고 싶어서라고 한다. 3년마다 기본계획을 수립하고 국가 인공지능 위원회와 정책 센터도 설립하게 된다고.


산업별로 보면 제조업은 AI 안전 위험성이 상대적으로 낮은 반면 보건의료는 생명과 직결되어 위험성이 크다고 평가하셨다. 금융은 이미 금감원이나 금융위원회의 자체 규제가 있고, 교육은 미래 세대에 미칠 영향을 고려해야 한다는 점도 강조하셨다. 이런 산업별 특성을 고려해 규제 정책도 차별화가 필요하다는 게 연구원님의 제안이었다. 특히 인상 깊었던 건 '고영향 AI'에 대한 논의였다. EU의 위험 기반 규제를 참고해 우리 기본법에도 도입된 개념인데 **생명이나 신체 안전에 직결되는 AI는 개발을 제한하는 방식**이다. 다만 현재 과태료가 3천만 원 수준이기에 큰 기업들에게는 실효성이 떨어질 수 있다는 우려도 있다고 한다.

마지막으로 연구원님께서는 앞으로는 개발자 중에서 규제자가 나와야 한다고 말씀하셨다. **법을 잘 아는 것 뿐만 아니라 개발도 할 수 있는 사람들이 제대로 된 규제를 만들 수 있다는 것.** 현재 세대는 가교 역할을 하고 있지만, 앞으로는 기술과 법을 모두 이해하는 전문가들이 필요하다는 말씀이 특히 와닿았다.

<br />

## Training AI to Train You : Your Trustworthy Coach
'Training AI to Train You' 세션에서는 조희주 연사님의 스파르탄 레이스 참가 경험부터 시작해 흥미진진한 이야기가 펼쳐졌다. 연사님께서는 6-7년간의 웨이트 트레이닝, 필라테스, F45 등 다양한 운동 경험을 바탕으로 **운동이 가져다준 신체적・정신적 변화에 대한 진솔한 이야기**를 들려주셨다. 이런 운동에 대한 애정이 결국 AI 트레이닝 코치 서비스를 만들게 된 계기가 되었다고 한다. '덕업일치(먹고 살기 위해 좋아하는 일을 한다)'라는 말처럼 좋아하는 일을 하면서도 사회적 가치를 만들어내고 싶었다는 솔직한 이야기가 와닿았다.

![](https://velog.velcdn.com/images/antraxmin/post/536b74f5-2b73-4b95-a992-a6906b8ae17f/image.png)


`Fitculator` 의 핵심 기능은 크게 두 가지인데, 먼저 AI 질문봇은 사람 코치에게는 물어보기 뻘쭘한 사소한 질문들도 편하게 할 수 있도록 설계했다고 한다. 특히 이 봇에게는 재미있는 페르소나를 부여했는데, MBTI로 치면 'T지만 따뜻하게 말해주는 F처럼 행동하는' 성격이라고 한다. 늘 대화 끝에 '화이팅!'을 붙여주는 센스도 있다고..!

두 번째 기능은 **AI 식단 피드백**이다. 사용자가 먹은 음식 사진과 설명을 올리면 AI가 영양학적 분석과 칼로리 정보는 물론, 같은 칼로리라도 정크푸드인지 아닌지까지 판단해준다고 한다. 다만 AI의 판단이 100% 정확하지 않을 수 있어서, 검수 과정을 거쳐 피드백을 제공하는 방식을 택했다는 점이 인상적이었다. 특히 강조하신 부분은 'Responsible AI'였다. **매력적인 기술보다는 사용자의 안전을, 빠른 구현보다는 신중한 접근을, 완벽한 자동화보다는 인간의 검토를 중요시한다는 철학**이 인상 깊었다. **"책임감 있는 AI는 모든 의사결정에서 시작된다"**는 마지막 말씀이 특히 기억에 남는다. 

이번 세션은 운동을 사랑하는 한 개발자가 AI 기술로 더 많은 사람들의 건강한 삶을 돕고 싶다는 진정성 있는 이야기였다. 나도 운동을 좋아하는 사람으로서 앞으로 Fitculator가 어떤 방식으로 AI와 운동을 접목시켜 사람들의 건강한 삶을 도울지 기대된다. 마침 연말이라 새해 운동 계획을 세우고 있었는데, 이런 믿음직한 AI 코치와 함께라면 더 즐겁게 시작할 수 있지 않을까 싶다. 

<br />

## LLM 시대의 Compliance: Security & Safety
Liner의 허훈님이 발표하신 'LLM 시대의 Compliance: Security & Safety' 세션에서는 **AI 서비스 운영에서 실제로 부딪히는 보안과 안전성 이슈**들을 다뤘다. 특히 사용자 데이터 컨트롤부터 AI의 안전한 운영까지, 실제 서비스 운영 경험을 바탕으로 한 인사이트가 인상적이었다.

먼저 기본적인 컴플라이언스 측면에서 Liner는 **사용자에게 직접 데이터 컨트롤 권한을 제공**한다고 한다. 설정 페이지에서 자신의 데이터가 AI 모델 학습에 활용될지 여부를 사용자가 직접 결정할 수 있게 했다는 점이 흥미로웠다. 이는 단순한 개인정보 보호를 넘어서 사용자에게 실질적인 데이터 주권을 부여한다는 점에서 의미가 있어 보였다. 안전성 측면에서는 특히 **유해한 사용자 입력을 제어하기 위한 여러 단계의 접근**이 인상적이었다. OpenAI의 모더레이션 API를 첫 번째 필터로 활용하고, 의심스러운 경우 Meta의 `Llama-guard` 로 한 번 더 검증하는 방식이다. 마치 추천 시스템처럼 가벼운 필터링부터 시작해서 점점 정밀한 검증을 하는 방식을 택했다는 게 흥미로웠다. 특히 `Llama-guard` 는 13가지 위험 유형을 정의하고 있는데, 이는 Anthropic Ethics Committee에서 정의한 기준을 따른다고 한다.


![](https://velog.velcdn.com/images/antraxmin/post/f7b60c7e-f9bd-474e-91f9-c3b3bc8fe2f0/image.png)





서비스 보안 측면에서는 **프롬프트 인젝션과 자일브레이크(jailbreak) 같은 어뷰징 대응**이 중요하다고 했다. 프롬프트 인젝션은 AI의 답변을 조작하려는 시도이고, 자일브레이크는 시스템을 무력화시켜 자유롭게 사용하려는 시도라고 한다. 특히 자일브레이크 시도가 빈번한데, 이를 막기 위해 Meta의 `Prompt-guard` 모델을 활용한다고 한다. 다만 이 모델이 키워드에 지나치게 민감하게 반응하는 단점이 있어 0.999 이상의 높은 임계값과 세심한 전처리가 필요하다는 점도 짚어주셨다. 실제 서비스에서는 이런 보안 조치들을 어떻게 조합하는지도 설명해주셨다. 사용자 입력이 들어오면 먼저 자일브레이크 여부를 체크하고, 그다음 모더레이션 API로 1차 필터링을, 필요한 경우 `Llama-guard` 로 2차 검증을 하는 방식이다. 특히 흥미로웠던 것은 이미지 노출 같은 경우 더 보수적인 기준을 적용한다는 점이었다. 예를 들어 성범죄 통계 같은 학술적 질문의 경우 텍스트 답변은 허용하되 이미지는 제한하는 식이다.

가장 인상 깊었던 건 **단순히 기술적인 방어가 아닌 서비스의 가치를 반영한 대응 방식**이었다. 예를 들어 AI에게 욕설을 하는 사용자에게 단순히 차단 메시지를 보여주는 대신 "더 친절하게 물어보면 더 좋은 답변을 받을 수 있어요"처럼 교육적인 메시지를 보여준다고 한다. 틱톡의 사례처럼 이런 **친절한 모더레이션이 오히려 사용자 충성도를 높일 수 있다는 점**도 흥미로웠다. 앞으로의 계획도 들려주셨는데, 내년에는 위반 사례 데이터베이스를 활용해 보안 감지 모델을 고도화하고, 게임의 제재 시스템처럼 반복 위반자에 대한 이용 제한 시스템도 도입할 예정이라고 한다. 또한 **완전 자동화된 시스템보다는 필요한 경우 사람의 검토가 가능한 운영 시스템을 구축하는 것**이 목표라고 한다.




> 이번 DevFest π를 통해 AI가 **기술적인 영역을 넘어 우리 사회에 어떤 영향을 미치고 어떻게 책임감 있게 발전해야 하는지** 깊이 생각해보는 시간이었다. 각각의 세션이 AI의 다른 측면을 보여주면서도 결국 하나의 메시지로 수렴됐다. 책임감 있는 AI는 더 이상 선택이 아닌 필수가 되어가고 있다.
>
> 다가오는 2025년, AI는 더욱 빠르게 발전하고 우리 삶 속으로 파고들 것이다. 이런 시점에서 이번 DevFest π는 우리가 나아가야 할 방향을 다시 한 번 생각해보게 해준 의미 있는 자리였다. **기술의 발전과 함께 그 책임도 함께 생각하는, 진정한 의미의 'Responsible AI'를 고민하는 개발자**가 되어야겠다는 다짐을 하게 된 하루였다.